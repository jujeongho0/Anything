{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"tfidf.ipynb","provenance":[],"authorship_tag":"ABX9TyNsJRCJ4WKC+QnGUJZElsQn"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"crWNMENXVRE8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618415657409,"user_tz":-540,"elapsed":8404,"user":{"displayName":"주정호","photoUrl":"","userId":"08532448573609813431"}},"outputId":"e6f35da1-4fcb-4987-b9a1-84f67a51406e"},"source":["!pip install konlpy\n","import codecs\n","from google.colab import drive\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.model_selection import train_test_split\n","from sklearn.naive_bayes import MultinomialNB\n","import re\n","from konlpy.tag import Okt\n","import numpy as np\n","\n","drive.mount('/content/gdrive')\n","fname = codecs.open(\"/content/gdrive/My Drive/word2vec/NAME.txt\", 'r', 'utf-8')\n","fexplane = codecs.open(\"/content/gdrive/My Drive/word2vec/EXPLANE.txt\", 'r', 'utf-8')\n","\n","labels = ['패브릭소파', '가죽소파', '풋스툴/쿠션형스툴', '소파베드',\n","          '더블/퀸/킹침대', '싱글/수퍼싱글+침대', '유아용침대', '로프트/2층침대', '디반침대', '어린이침대',\n","          '옷장', '어린이옷장',\n","          '바테이블', '커피테이블/보조테이블', '식탁', '책상/컴퓨터책상', '화장대', '어린이테이블', '기저귀교환대',   \n","          '식탁의자', '바의자', '카페의자','사무용의자', '스툴/벤치', '어린이의자', '영아용의자', '패브릭암체어', '가죽암체어', '라탄암체어', '리클라이너',\n","          '수납가구', \n","          '커튼', '샤워커튼', '블라인드'] # 34개\n","\n","\n","def clean_korean_documents(documents):\n","    \n","    #텍스트 정제 (특수기호 제거)\n","    for i, document in enumerate(documents):\n","        document = re.sub(r'[^ ㄱ-ㅣ가-힣]', '', document) #특수기호 제거, 정규 표현식\n","        documents[i] = document\n","    \n","    #텍스트 정제 (형태소 추출)\n","    for i, document in enumerate(documents):\n","        okt = Okt()\n","        clean_words = []\n","        for word in okt.pos(document): #어간 추출\n","            if word[1] in ['Noun', 'Verb', 'Adjective']:\n","                clean_words.append(word[0])\n","        document = ' '.join(clean_words)\n","        documents[i] = document\n","    \n","    return documents\n","\n","y_data = []\n","while True:    \n","    line = flabel.readline()\n","    if not line: break\n","    line = line.rstrip('\\n')\n","    line = line.rstrip('\\r')\n","    line = furniture_category[int(line)]\n","    y_data.append((line))\n","\n","x_data = []\n","while True:\n","    line = fexplane.readline()\n","    if not line: break\n","    line = line.rstrip('\\n')\n","    x_data.append(line)\n","\n","x_data = clean_korean_documents(x_data)\n","transformer = TfidfVectorizer()\n","transformer.fit(x_data)\n","x_data = transformer.transform(x_data)\n","\n","x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.1, random_state=777)\n","\n","model = MultinomialNB(alpha=1.0)\n","model.fit(x_train, y_train)\n","print(model.score(x_test, y_test)) \n","\n","model.save(\"/content/gdrive/My Drive/word2vec/multinomialNB.h5\")\n","\n","fname.close()\n","fexplane.close()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","0.7303571428571428\n"],"name":"stdout"}]}]}